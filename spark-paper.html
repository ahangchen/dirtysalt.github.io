<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>Spark: Cluster Computing with Working Sets</title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="http://dirtysalt.info/css/favicon.ico" />
<link rel="stylesheet" type="text/css" href="./css/site.css" />
</head>
<body>
<div id="content">
<h1 class="title">Spark: Cluster Computing with Working Sets</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">1. Abstract</a></li>
<li><a href="#orgheadline2">2. Introduction</a></li>
<li><a href="#orgheadline6">3. Programming Model</a>
<ul>
<li><a href="#orgheadline3">3.1. Resilient Distributed Datasets (RDDs)</a></li>
<li><a href="#orgheadline4">3.2. Parallel Operations</a></li>
<li><a href="#orgheadline5">3.3. Shared Variables</a></li>
</ul>
</li>
<li><a href="#orgheadline7">4. Examples</a></li>
<li><a href="#orgheadline11">5. Implementation</a>
<ul>
<li><a href="#orgheadline8">5.1. RDD</a></li>
<li><a href="#orgheadline9">5.2. Shared Variables</a></li>
<li><a href="#orgheadline10">5.3. Interpret Intergation</a></li>
</ul>
</li>
<li><a href="#orgheadline12">6. Results</a></li>
<li><a href="#orgheadline13">7. Related Work</a></li>
<li><a href="#orgheadline14">8. Discussion and Future Work</a></li>
</ul>
</div>
</div>
<p>
<a href="https://www.usenix.org/legacy/events/hotcloud10/tech/full_papers/Zaharia.pdf">https://www.usenix.org/legacy/events/hotcloud10/tech/full_papers/Zaharia.pdf</a> @ 2010
</p>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1"><span class="section-number-2">1</span> Abstract</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper fo- cuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools.（迭代计算和交互式分析）</li>
<li>We propose a new framework called Spark that supports these applica- tions while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost.（RDD是一个只读对象的集合，并且会被分割成为多个partition，分布在多个机器上。如果一个partition丢失的话可以重算这个partition)</li>
<li>Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-2">
<h2 id="orgheadline2"><span class="section-number-2">2</span> Introduction</h2>
<div class="outline-text-2" id="text-2">
<p>
In this paper, we focus on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes two use cases where we have seen Hadoop users report that MapReduce is deficient:
</p>
<ul class="org-ul">
<li><b>Iterative jobs</b>: Many common machine learning algo- rithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient de- scent). While each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penalty.</li>
<li><b>Interactive analytics</b>: Hadoop is often used to run ad-hoc exploratory queries on large datasets, through SQL interfaces such as Pig and Hive. Ideally, a user would be able to load a dataset of interest into memory across a number of machines and query it re- peatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk.</li>
</ul>

<hr  />
<ul class="org-ul">
<li>The main abstraction in Spark is that of a resilient dis- tributed dataset (RDD), which represents a read-only col- lection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Users can explicitly cache an RDD in memory across machines and reuse it in multiple MapReduce-like parallel operations. （RDD可以存储在内存中，并且可以重复使用）</li>
<li>RDDs achieve fault tolerance through a notion of lineage: if a partition of an RDD is lost, the RDD has enough infor- mation about how it was derived from other RDDs to be able to rebuild just that partition.（RDD通过lineage来达到fault-tolerant. 一个RDD内部包含足够信息来重新构造自己。因为RDD是只读的，所以这个信息只需要是input + sequence of transformatios)</li>
<li>Although RDDs are not a general shared memory abstraction, they represent a sweet-spot between expressivity on the one hand and scalability and reliability on the other hand, and we have found them well-suited for a variety of applications.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline6" class="outline-2">
<h2 id="orgheadline6"><span class="section-number-2">3</span> Programming Model</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>To use Spark, developers write a driver program that im- plements the high-level control flow of their application and launches various operations in parallel.</li>
<li>Spark pro- vides two main abstractions for parallel programming: resilient distributed datasets and parallel operations on these datasets (invoked by passing a function to apply on a dataset).</li>
<li>In addition, Spark supports two restricted types of shared variables that can be used in functions running on the cluster, which we shall explain later.（共享变量）</li>
</ul>
</div>

<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">3.1</span> Resilient Distributed Datasets (RDDs)</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>A resilient distributed dataset (RDD) is a read-only col- lection of objects partitioned across a set of machines that can be rebuilt if a partition is lost.</li>
<li>The elements of an RDD need not exist in physical storage; instead, a handle to an RDD contains enough information to compute the RDD starting from data in reliable storage. This means that RDDs can always be reconstructed if nodes fail.（每个RDD包含如何从data in reliable storage经过一系列变化转换过来）</li>
<li>In Spark, each RDD is represented by a Scala object. Spark lets programmers construct RDDs in four ways:（RDD能够通过下面4种方式来构造）
<ul class="org-ul">
<li>From a file in a shared file system, such as the Hadoop Distributed File System (HDFS).（从HDFS中读取）</li>
<li>By “parallelizing” a Scala collection (e.g., an array) in the driver program, which means dividing it into a number of slices that will be sent to multiple nodes.（将driver中的collection切片）</li>
<li>By transforming an existing RDD. A dataset with ele- ments of type A can be transformed into a dataset with elements of type B using an operation called flatMap, which passes each element through a user-provided function of type A ⇒ List[B]. Other transforma- tions can be expressed using flatMap, including map (pass elements through a function of type A ⇒ B) and filter (pick elements matching a predicate).（经过transformation)</li>
<li>By changing the persistence of an existing RDD. By default, RDDs are lazy and ephemeral. That is, par- titions of a dataset are materialized on demand when they are used in a parallel operation (e.g., by passing a block of a file through a map function), and are dis- carded from memory after use. However, a user can alter the persistence of an RDD through two actions:
<ul class="org-ul">
<li>The cache action leaves the dataset lazy, but hints that it should be kept in memory after the first time it is computed, because it will be reused. We note that our cache action is only a hint: if there is not enough memory in the cluster to cache all partitions of a dataset, Spark will recompute them when they are used. We chose this design so that Spark programs keep work- ing (at reduced performance) if nodes fail or if a dataset is too big. This idea is loosely analogous to virtual memory.(cache可以用来提示将RDD缓存在内存中，以便被后面计算重复使用。如果空间不够的话那么会丢弃而下次需要的时候重新计算，类似虚拟内存）</li>
<li>The save action evaluates the dataset and writes it to a distributed filesystem such as HDFS. The saved version is used in future operations on it.（save可以用来将RDD持久化到磁盘上）</li>
</ul></li>
<li>We also plan to extend Spark to support other levels of persistence (e.g., in-memory replication across multiple nodes). Our goal is to let users trade off between the cost of storing an RDD, the speed of accessing it, the proba- bility of losing part of it, and the cost of recomputing it.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-3">
<h3 id="orgheadline4"><span class="section-number-3">3.2</span> Parallel Operations</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Several parallel operations can be performed on RDDs:
</p>
<ul class="org-ul">
<li>reduce: Combines dataset elements using an associa- tive function to produce a result at the driver program.</li>
<li>collect: Sends all elements of the dataset to the driver program. For example, an easy way to update an array in parallel is to parallelize, map and collect the array.</li>
<li>foreach: Passes each element through a user provided function. This is only done for the side effects of the function (which might be to copy data to another sys- tem or to update a shared variable as explained below).</li>
</ul>

<p>
We note that Spark does not currently support a grouped reduce operation as in MapReduce; reduce re- sults are only collected at one process (the driver). We plan to support grouped reductions in the future using a “shuffle” transformation on distributed datasets, as de- scribed in Section 7.（没有shuffle是显然不行的）
</p>
</div>
</div>

<div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5"><span class="section-number-3">3.3</span> Shared Variables</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>Programmers invoke operations like map, filter and re- duce by passing closures (functions) to Spark. As is typi- cal in functional programming, these closures can refer to variables in the scope where they are created. Normally, when Spark runs a closure on a worker node, these vari- ables are copied to the worker.（closure使用的变量会被复制到worker上）</li>
<li>However, Spark also lets programmers create two restricted types of shared vari- ables to support two simple but common usage patterns:
<ul class="org-ul">
<li>Broadcast variables: If a large read-only piece of data (e.g., a lookup table) is used in multiple parallel op- erations, it is preferable to distribute it to the workers only once instead of packaging it with every closure. Spark lets the programmer create a “broadcast vari-able” object that wraps the value and ensures that it is only copied to each worker once.（广播变量，类似Hadoop的distributed cache)</li>
<li>Accumulators: These are variables that workers can only “add” to using an associative operation, and that only the driver can read. They can be used to im- plement counters as in MapReduce and to provide a more imperative syntax for parallel sums. Accumu- lators can be defined for any type that has an “add” operation and a “zero” value. Due to their “add-only” semantics, they are easy to make fault-tolerant.(累加器，类似Hadoop的counter)</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline7" class="outline-2">
<h2 id="orgheadline7"><span class="section-number-2">4</span> Examples</h2>
</div>
<div id="outline-container-orgheadline11" class="outline-2">
<h2 id="orgheadline11"><span class="section-number-2">5</span> Implementation</h2>
<div class="outline-text-2" id="text-5">

<div class="figure">
<p><img src="./images/spark-cluster-overview.png" alt="spark-cluster-overview.png" />
</p>
</div>
</div>

<div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8"><span class="section-number-3">5.1</span> RDD</h3>
<div class="outline-text-3" id="text-5-1">
<p>
The core of Spark is the implementation of resilient dis- tributed datasets. As an example, suppose that we define a cached dataset called cachedErrs representing error messages in a log file, and that we count its elements us- ing map and reduce, as in Section 3.1:
</p>

<div class="org-src-container">

<pre class="src src-Scala">val file = spark.textFile("hdfs://...")
val errs = file.filter(_.contains("ERROR"))
val cachedErrs = errs.cache()
val ones = cachedErrs.map(_ =&gt; 1)
val count = ones.reduce(_+_)
</pre>
</div>

<p>
These datasets will be stored as a chain of objects cap- turing the lineage of each RDD, shown in Figure 1. Each dataset object contains a pointer to its parent and informa- tion about how the parent was transformed.
</p>


<div class="figure">
<p><img src="./images/spark-rdd-code-examples.png" alt="spark-rdd-code-examples.png" />
</p>
</div>

<p>
#note: rdd = direct input + function as closure
</p>

<hr  />

<p>
Internally, each RDD object implements the same sim- ple interface, which consists of three operations:（RDD interface)
</p>
<ul class="org-ul">
<li>getPartitions, which returns a list of partition IDs.（这个RDD有哪些partitions)</li>
<li>getIterator(partition), which iterates over a partition.(遍历partition获取数据）</li>
<li>getPreferredLocations(partition), which is used for task scheduling to achieve data locality.（这个partition存储在哪些地方，这样可以将人任务分发到上面提高data locality)</li>
</ul>
<p>
When a parallel operation is invoked on a dataset, Spark creates a task to process each partition of the dataset and sends these tasks to worker nodes. <b>We try to send each task to one of its preferred locations using a technique called delay scheduling.</b> Once launched on a worker, each task calls getIterator to start reading its partition.(通过将task放置到partition所在的位置称为延迟调度。一旦worker启动之后获取partition的遍历器来读取数据）
</p>

<p>
The different types of RDDs differ only in how they implement the RDD interface. For example,
</p>
<ul class="org-ul">
<li>for a Hdfs- TextFile, the partitions are block IDs in HDFS, their pre- ferred locations are the block locations, and getIterator opens a stream to read a block.</li>
<li>In a MappedDataset, the partitions and preferred locations are the same as for the parent, but the iterator applies the map function to ele- ments of the parent.</li>
<li>Finally, in a CachedDataset, the getIterator method looks for a locally cached copy of a transformed partition, and each partition’s preferred loca- tions start out equal to the parent’s preferred locations, but get updated after the partition is cached on some node to prefer reusing that node.</li>
</ul>
<p>
This design makes faults easy to handle: if a node fails, its partitions are re-read from their parent datasets and eventually cached on other nodes.
</p>

<p>
Finally, shipping tasks to workers requires shipping closures to them—both the closures used to define a dis- tributed dataset, and closures passed to operations such as reduce. To achieve this, we rely on the fact that Scala clo- sures are Java objects and can be serialized using Java se- rialization; this is a feature of Scala that makes it relatively straightforward to send a computation to another machine. Scala’s built-in closure implementation is not ideal, how- ever, because we have found cases where a closure object references variables in the closure’s outer scope that are not actually used in its body. We have filed a bug report about this, but in the meantime, we have solved the issue by performing a static analysis of closure classes’ byte- code to detect these unused variables and set the corre- sponding fields in the closure object to null. We omit the details of this analysis due to lack of space.（通过对closure做序列化将task散布到worker上面）
</p>
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-3">
<h3 id="orgheadline9"><span class="section-number-3">5.2</span> Shared Variables</h3>
<div class="outline-text-3" id="text-5-2">
<p>
The two types of shared variables in Spark, broadcast variables and accumulators, are imple- mented using classes with custom serialization formats.
</p>
<ul class="org-ul">
<li>When one creates a broadcast variable b with a value v, v is saved to a file in a shared file system. The serialized form of b is a path to this file. When b’s value is queried on a worker node, Spark first checks whether v is in a local cache, and reads it from the file system if it isn’t. We initially used HDFS to broadcast variables, but we are developing a more efficient streaming broadcast system.（将HDFS当作共享文件系统，广播数据存储在HDFS上面，而广播变量就是HDFS的文件路径）</li>
<li>Accumulators are implemented using a different “se- rialization trick.” Each accumulator is given a unique ID when it is created. When the accumulator is saved, its serialized form contains its ID and the “zero” value for its type. On the workers, a separate copy of the accu- mulator is created for each thread that runs a task using thread-local variables, and is reset to zero when a task be- gins. After each task runs, the worker sends a message to the driver program containing the updates it made to var- ious accumulators. The driver applies updates from each partition of each operation only once to prevent double- counting when tasks are re-executed due to failures.（累加器变量由driver分配ID，然后各个worker汇报在自己在这个ID上的增量）</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline10" class="outline-3">
<h3 id="orgheadline10"><span class="section-number-3">5.3</span> Interpret Intergation</h3>
</div>
</div>
<div id="outline-container-orgheadline12" class="outline-2">
<h2 id="orgheadline12"><span class="section-number-2">6</span> Results</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>Distributed Shared Memory</li>
<li>Cluster Computing Frameworks</li>
<li><b>Language Integration</b></li>
<li><b>Lineage</b></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline13" class="outline-2">
<h2 id="orgheadline13"><span class="section-number-2">7</span> Related Work</h2>
</div>
<div id="outline-container-orgheadline14" class="outline-2">
<h2 id="orgheadline14"><span class="section-number-2">8</span> Discussion and Future Work</h2>
<div class="outline-text-2" id="text-8">
<p>
In future work, we plan to focus on four areas:
</p>
<ol class="org-ol">
<li>Formally characterize the properties of RDDs and Spark’s other abstractions, and their suitability for var- ious classes of applications and workloads.</li>
<li>Enhance the RDD abstraction to allow programmers to trade between storage cost and re-construction cost.</li>
<li>Define new operations to transform RDDs, including a “shuffle” operation that repartitions an RDD by a given key. Such an operation would allow us to im- plement group-bys and joins.</li>
<li>Provide higher-level interactive interfaces on top of the Spark interpreter, such as SQL and R shells.</li>
</ol>
</div>
</div>
</div>
<!-- DISQUS BEGIN --><div id="disqus_thread"></div><script type="text/javascript">/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * *//* required: replace example with your forum shortname  */var disqus_shortname = 'dirlt';var disqus_identifier = 'spark-paper.html';var disqus_title = 'spark-paper.html';var disqus_url = 'http://dirtysalt.info/spark-paper.html';/* * * DON'T EDIT BELOW THIS LINE * * */(function() {var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><!-- DISQUS END --></body>
</html>
